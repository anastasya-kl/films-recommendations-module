{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11026ef4-2d76-4b36-8f51-2937183c9d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "import re\n",
    "import spacy\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d4bdad-bd77-492a-904d-ea36e199d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def add_clear(tokens):\n",
    "    l = len(tokens)\n",
    "    for t in range(l):\n",
    "        splited = tokens[t].split()\n",
    "        if len(splited) == 1: pass\n",
    "        else:\n",
    "            tokens[t] = splited[0]\n",
    "            for w in splited[1:]: tokens.append(w)\n",
    "    return tokens\n",
    "\n",
    "def process_text_to_tokens(text,  stop_words, target_language='en'):\n",
    "    text = text.replace('/', ' ').replace('-', ' ')\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = ' '.join(text.split()) \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    translator = Translator()\n",
    "    tokens = [translator.translate(token, dest=target_language).text for token in tokens]\n",
    "    tokens = [token.lower() for token in tokens] \n",
    "    tokens = add_clear(tokens)\n",
    "    tokens = list(set(tokens))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    str_tokens = ''\n",
    "    for token in tokens: str_tokens += token + ' '\n",
    "    str_tokens = ' '.join(str_tokens.split())\n",
    "    if not str_tokens:\n",
    "        return None\n",
    "    return str_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bf0361-de2c-4760-8ba8-7c3f8291b517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tokens_column(stop_words, dbname=\"fluxmain.db\"):\n",
    "    try:\n",
    "        with sq.connect(dbname) as con:\n",
    "            cur = con.cursor()\n",
    "\n",
    "            # Отримати всі дані зі стовпця caption та id таблиці films\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT id, caption, tokens\n",
    "                FROM films\n",
    "            \"\"\")\n",
    "            \n",
    "            films_data = cur.fetchall()\n",
    "\n",
    "            for film_id, caption, tokens in films_data:\n",
    "                if tokens is None:\n",
    "                    tokens = process_text_to_tokens(caption, stop_words)\n",
    "                    cur.execute(\"\"\"\n",
    "                        UPDATE films\n",
    "                        SET tokens = ?\n",
    "                        WHERE id = ?\n",
    "                    \"\"\", (tokens, film_id))\n",
    "\n",
    "                    con.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"Помилка підключення до сервера: {e}\")\n",
    "        update_tokens_column(stop_words)\n",
    "\n",
    "\n",
    "# Виклик функції для оновлення стовпця tokens\n",
    "update_tokens_column(stop_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
